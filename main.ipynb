{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\Desktop\\Projects\\NUS-ISS\\text\\nusiss-project-presidential-absa-system\\myVenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "c:\\Users\\user\\Desktop\\Projects\\NUS-ISS\\text\\nusiss-project-presidential-absa-system\\myVenv\\lib\\site-packages\\spacy_transformers\\layers\\hf_shim.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self._model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "c:\\Users\\user\\Desktop\\Projects\\NUS-ISS\\text\\nusiss-project-presidential-absa-system\\myVenv\\lib\\site-packages\\thinc\\shims\\pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "c:\\Users\\user\\Desktop\\Projects\\NUS-ISS\\text\\nusiss-project-presidential-absa-system\\myVenv\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\Desktop\\Projects\\NUS-ISS\\text\\nusiss-project-presidential-absa-system\\myVenv\\lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "c:\\Users\\user\\Desktop\\Projects\\NUS-ISS\\text\\nusiss-project-presidential-absa-system\\myVenv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from scraper.reddit_scraper import get_reddit_object, get_qa_object, \\\n",
    "                                   get_trump_reddit_posts_and_comments, \\\n",
    "                                    get_kamala_reddit_posts_and_comments, \\\n",
    "                                    read_paths_create_df\n",
    "from scraper.youtube_scraper import scrape_youtube\n",
    "from utilities.util import create_folder_if_not_exists\n",
    "from preprocessor.preprocess import rename_df_cols, set_post_title_as_parent_comment_if_na, \\\n",
    "                                    unify_youtube_and_reddit_comments, combine_reddit_comment_and_post_df,\\\n",
    "                                    preprocess_dataset, split_comments_to_sentence,\\\n",
    "                                    assign_level\n",
    "from preprocessor.coreference_resolution import coref_resolve\n",
    "from preprocessor.subjectivity_classifier import is_subjective\n",
    "\n",
    "from model.prediction import predict_with_models, AspectBasedSentimentModel\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, \\\n",
    "                        AutoModelForSeq2SeqLM, BartForConditionalGeneration\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter file paths if necessary\n",
    "REDDIT_SAVE_DIR = './data/reddit/'\n",
    "YOUTUBE_SAVE_DIR = './data/youtube/'\n",
    "FINAL_FILE_SAVE_DIR = './data/'\n",
    "\n",
    "CONSOLIDATED_REDDIT_COMMENT_SAVE_FILE = f'{REDDIT_SAVE_DIR}/consolidated_reddit_comments.csv'\n",
    "CONSOLIDATED_REDDIT_POST_SAVE_FILE = f'{REDDIT_SAVE_DIR}/consolidated_reddit_posts.csv'\n",
    "YOUTUBE_FILE_NAME = 'youtube_comments.csv'\n",
    "\n",
    "COMBINED_COMMENT_LEVEL_FILE = 'comment_level.csv'\n",
    "COMBINED_SENTENCE_LEVEL_FILE = 'sentence_level.csv'\n",
    "\n",
    "ENTITY_MODEL = 'destonedbob/nusiss-election-project-entity-model-distilbert-base-cased'\n",
    "ASPECT_MODEL_DISTIL = './model/multilabel_aspect_distil_4epochs_lr3e-5_without_test_set_split_keep_same_sent_together.pth'\n",
    "ASPECT_MODEL_SEQ2SEQ = 'destonedbob/nusiss-election-project-aspect-seq2seq-model-facebook-bart-large'\n",
    "SENTIMENT_MODEL_DISTIL = './model/sentiment_model_val_acc_6162_lr4.5e-5_wtdecay_1e-4_epochs4_256_256_256_256_smoothed_weight_warmup_and_reducelr_freeze4layers.pth'\n",
    "SENTIMENT_MODEL_SEQ2SEQ = 'destonedbob/nusiss-election-project-sentiment-seq2seq-model-facebook-bart-large'\n",
    "DISTILBERT_BASE_CASED = 'distilbert-base-cased'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_idx_map = {k:v for v, k in enumerate(['kamala', 'trump', 'others'])}\n",
    "idx_entity_map = {v:k for k, v in entity_idx_map.items()}\n",
    "aspect_idx_map = {k:v for v, k in enumerate(['campaign', 'communication', 'competence', 'controversies',\n",
    "       'ethics and integrity', 'leadership', \n",
    "       'personality trait', 'policies', 'political ideology',\n",
    "       'public image', 'public service record',\n",
    "       'relationships and alliances', 'voter sentiment', 'others'])}\n",
    "idx_aspect_map = {v:k for k, v in aspect_idx_map.items()}\n",
    "sentiment_idx_map = {k:v for v, k in enumerate(['negative', 'neutral', 'positive'])}\n",
    "idx_sentiment_map = {v:k for k, v in sentiment_idx_map.items()}\n",
    "idx_sentiment_map2 = {v-1:k for k, v in sentiment_idx_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [REDDIT_SAVE_DIR, YOUTUBE_SAVE_DIR, FINAL_FILE_SAVE_DIR]:\n",
    "    create_folder_if_not_exists(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "reddit = get_reddit_object()\n",
    "qa_obj = get_qa_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDDIT_POST_TO_SCRAPE = 1 # Change to alter number of posts to scrape comments from\n",
    "trump_post_df, trump_comment_df = get_trump_reddit_posts_and_comments(reddit, qa_obj, REDDIT_SAVE_DIR, max_posts_to_collect=REDDIT_POST_TO_SCRAPE)\n",
    "kamala_post_df, kamala_comment_df = get_kamala_reddit_posts_and_comments(reddit, qa_obj, REDDIT_SAVE_DIR, max_posts_to_collect=REDDIT_POST_TO_SCRAPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_reddit_comment_file_paths = [REDDIT_SAVE_DIR+file for file in os.listdir(REDDIT_SAVE_DIR) if '_comment_data_' in file and 'test' not in file]\n",
    "scraped_reddit_post_file_paths = [REDDIT_SAVE_DIR+file for file in os.listdir(REDDIT_SAVE_DIR) if '_post_data_' in file and 'test' not in file]\n",
    "\n",
    "scraped_reddit_comment_df = read_paths_create_df(scraped_reddit_comment_file_paths)\n",
    "scraped_reddit_comment_df.to_csv(CONSOLIDATED_REDDIT_COMMENT_SAVE_FILE, index=False)\n",
    "scraped_reddit_post_df = read_paths_create_df(scraped_reddit_post_file_paths)\n",
    "scraped_reddit_post_df.to_csv(CONSOLIDATED_REDDIT_POST_SAVE_FILE, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_reddit_comment_df = pd.read_csv(CONSOLIDATED_REDDIT_COMMENT_SAVE_FILE)\n",
    "scraped_reddit_post_df = pd.read_csv(CONSOLIDATED_REDDIT_POST_SAVE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Scrape TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YouTube comments have been saved to ./data/youtube/youtube_comments.csv\n"
     ]
    }
   ],
   "source": [
    "youtube_comment_df = scrape_youtube(YOUTUBE_SAVE_DIR+YOUTUBE_FILE_NAME, videos_to_scrape=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_comment_df = pd.read_csv(YOUTUBE_SAVE_DIR+YOUTUBE_FILE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unifying Reddit + Youtube Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit_comment = rename_df_cols(scraped_reddit_comment_df, 'reddit_comments')\n",
    "df_reddit_post = rename_df_cols(scraped_reddit_post_df, 'reddit_posts')\n",
    "df_youtube = rename_df_cols(youtube_comment_df, 'youtube')\n",
    "\n",
    "df_reddit_comment = df_reddit_comment.drop_duplicates('comment_id', keep='first')\n",
    "df_youtube = df_youtube.drop_duplicates('comment_id', keep='first')\n",
    "df_reddit_post = df_reddit_post.drop_duplicates('post_id', keep='first')\n",
    "\n",
    "df_reddit = combine_reddit_comment_and_post_df(df_reddit_comment, df_reddit_post)\n",
    "combined_df = unify_youtube_and_reddit_comments(df_reddit, df_youtube)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess and reduce scraped comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below preprocess_dataset function will handle:\n",
    "1) Fix unicode encoding issues using the html and unicodedata library \n",
    "2) Remove non-english text using facebook/fasttext-language-identification\n",
    "3) Replace misformatted punctuation e.g. '“', '”'\n",
    "4) Remove non-human readable characters using regex, e.g. empty string characters\n",
    "5) Replace URL with placeholder using regex [URL]\n",
    "6) Assign level to comment (e.g. root comment = 1, reply to root = 2). Will be used for coreference resolution later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess as above\n",
    "combined_df = preprocess_dataset(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in output string: That's fine and all, but what about that one time when they loaded up a C17 with a bunch of \"supplies\" that were being \"flown to North Carolina\" so that Kamala could come and grab a photo op, only to have the \"supplies\" unloaded from the plane? [ NEXT_COMMENT] You say, \"that one time when they loaded up a C17 with a bunch of \"supplies\" that were being \"flown to North Carolina\" so that Kamala could come and grab a photo op, only to have the \"supplies\" unloaded from the plane? [ \" as if that one time when they loaded up a C17 with a bunch of \"supplies\" that were being \"flown to North Carolina\" so that Kamala could come and grab a photo op, only to have the \"supplies\" unloaded from the plane? [ were quite some time ago and one amongst many. Did you by chance Huff Dust Off?Or did you by chance partake in eating the dogs and cats in Ohio? I heard they were all fed psychedelics before they were cooked so that might explain it...\n"
     ]
    }
   ],
   "source": [
    "combined_df = coref_resolve(combined_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_df.to_csv(FINAL_FILE_SAVE_DIR+COMBINED_COMMENT_LEVEL_FILE, index=False, encoding='utf-8-sig')\n",
    "combined_df = pd.read_csv(FINAL_FILE_SAVE_DIR+COMBINED_COMMENT_LEVEL_FILE, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_timestamp</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_timestamp</th>\n",
       "      <th>number_of_comment_votes</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lrt56j4</td>\n",
       "      <td>1g34hu5</td>\n",
       "      <td>Bryan Cranston campaigning for Kamala Harris i...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>It looks like this post is about Politics. Var...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>It looks like this post is about Politics.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lrt56j4</td>\n",
       "      <td>1g34hu5</td>\n",
       "      <td>Bryan Cranston campaigning for Kamala Harris i...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>It looks like this post is about Politics. Var...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Various methods of filtering out content relat...</td>\n",
       "      <td>It looks like this post is about Politics.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lrt56j4</td>\n",
       "      <td>1g34hu5</td>\n",
       "      <td>Bryan Cranston campaigning for Kamala Harris i...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>It looks like this post is about Politics. Var...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Please [contact the moderators of this subredd...</td>\n",
       "      <td>Various methods of filtering out content relat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lrt56j4</td>\n",
       "      <td>1g34hu5</td>\n",
       "      <td>Bryan Cranston campaigning for Kamala Harris i...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>It looks like this post is about Politics. Var...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>*</td>\n",
       "      <td>Please [contact the moderators of this subredd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lrt9w4u</td>\n",
       "      <td>1g34hu5</td>\n",
       "      <td>Bryan Cranston campaigning for Kamala Harris i...</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>\"I am the one who votes!\"</td>\n",
       "      <td>2024-10-14 00:00:00+00:00</td>\n",
       "      <td>4509</td>\n",
       "      <td>\"I am the one who votes!\"</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id  post_id                                         post_title  \\\n",
       "0    lrt56j4  1g34hu5  Bryan Cranston campaigning for Kamala Harris i...   \n",
       "1    lrt56j4  1g34hu5  Bryan Cranston campaigning for Kamala Harris i...   \n",
       "2    lrt56j4  1g34hu5  Bryan Cranston campaigning for Kamala Harris i...   \n",
       "3    lrt56j4  1g34hu5  Bryan Cranston campaigning for Kamala Harris i...   \n",
       "4    lrt9w4u  1g34hu5  Bryan Cranston campaigning for Kamala Harris i...   \n",
       "\n",
       "             post_timestamp parent_comment_id parent_comment  \\\n",
       "0 2024-10-14 00:00:00+00:00               NaN                  \n",
       "1 2024-10-14 00:00:00+00:00               NaN                  \n",
       "2 2024-10-14 00:00:00+00:00               NaN                  \n",
       "3 2024-10-14 00:00:00+00:00               NaN                  \n",
       "4 2024-10-14 00:00:00+00:00               NaN                  \n",
       "\n",
       "                                             comment  \\\n",
       "0  It looks like this post is about Politics. Var...   \n",
       "1  It looks like this post is about Politics. Var...   \n",
       "2  It looks like this post is about Politics. Var...   \n",
       "3  It looks like this post is about Politics. Var...   \n",
       "4                          \"I am the one who votes!\"   \n",
       "\n",
       "          comment_timestamp  number_of_comment_votes  \\\n",
       "0 2024-10-14 00:00:00+00:00                        1   \n",
       "1 2024-10-14 00:00:00+00:00                        1   \n",
       "2 2024-10-14 00:00:00+00:00                        1   \n",
       "3 2024-10-14 00:00:00+00:00                        1   \n",
       "4 2024-10-14 00:00:00+00:00                     4509   \n",
       "\n",
       "                                            sentence  \\\n",
       "0         It looks like this post is about Politics.   \n",
       "1  Various methods of filtering out content relat...   \n",
       "2  Please [contact the moderators of this subredd...   \n",
       "3                                                  *   \n",
       "4                          \"I am the one who votes!\"   \n",
       "\n",
       "                                   previous_sentence  \n",
       "0                                               None  \n",
       "1         It looks like this post is about Politics.  \n",
       "2  Various methods of filtering out content relat...  \n",
       "3  Please [contact the moderators of this subredd...  \n",
       "4                                               None  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df = split_comments_to_sentence(combined_df[~combined_df.comment.isna()])\n",
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove objective sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using cffl/bert-base-styleclassification-subjective-neutral\n",
    "sentence_df['is_subjective'] = sentence_df.sentence.apply(is_subjective) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = sentence_df[sentence_df.is_subjective == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_df.to_csv(FINAL_FILE_SAVE_DIR+COMBINED_SENTENCE_LEVEL_FILE, index=False, encoding='utf-8-sig')\n",
    "sentence_df = pd.read_csv(FINAL_FILE_SAVE_DIR+COMBINED_SENTENCE_LEVEL_FILE, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = sentence_df.copy()\n",
    "df = sentence_df.sample(5, random_state=666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict_with_models is a function that consolidates the below code. As long as you pass it a df with \"sentence\" column, it should work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>final_aspect_labels</th>\n",
       "      <th>entity_category</th>\n",
       "      <th>final_aspect_categories</th>\n",
       "      <th>final_sentiment_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did people honestly think trump actually worke...</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>trump</td>\n",
       "      <td>controversies</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Better than Kamala any day of the week.</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>kamala</td>\n",
       "      <td>voter sentiment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who gives a fuck</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>others</td>\n",
       "      <td>voter sentiment</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hillary is not in Prison is she?</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>others</td>\n",
       "      <td>others</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence entity_ids  \\\n",
       "0  Did people honestly think trump actually worke...  [0, 1, 0]   \n",
       "1            Better than Kamala any day of the week.  [1, 0, 0]   \n",
       "2                                   Who gives a fuck  [0, 0, 1]   \n",
       "3                   Hillary is not in Prison is she?  [0, 0, 1]   \n",
       "\n",
       "                          final_aspect_labels entity_category  \\\n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]           trump   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]          kamala   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]          others   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]          others   \n",
       "\n",
       "  final_aspect_categories  final_sentiment_prediction  \n",
       "0           controversies                          -1  \n",
       "1         voter sentiment                           1  \n",
       "2         voter sentiment                          -1  \n",
       "3                  others                           0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_with_models(df[['sentence']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(ENTITY_MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(ENTITY_MODEL)\n",
    "model.to('cuda')\n",
    "\n",
    "def get_probabilities(texts, score=False):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs.to('cuda')\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.sigmoid(logits)\n",
    "    if not score:\n",
    "        return np.array(list(map(lambda x: 1 if x > 0.65 else 0, probabilities.cpu().detach().numpy()[0].tolist())))\n",
    "    else:\n",
    "        return probabilities.cpu().detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entity_ids'] = df.sentence.apply(get_probabilities)\n",
    "df_columns = df.columns.tolist()\n",
    "# Expand the DataFrame for each entity\n",
    "expanded_rows = []\n",
    "for index, row in df.iterrows():\n",
    "    entity_labels = row['entity_ids']\n",
    "    if entity_labels[0] == 1:\n",
    "        dict_row = dict()\n",
    "        for col in df.columns:\n",
    "            dict_row[col] = row[col]\n",
    "            dict_row['entity_category'] = 'kamala'\n",
    "            dict_row['entity_id'] = entity_idx_map['kamala']\n",
    "\n",
    "        expanded_rows.append(dict_row)\n",
    "        # expanded_rows.append({\n",
    "        #     'comment': row['comment'],\n",
    "        #     'parent_comment': row['parent_comment'],\n",
    "        #     'comment_id': row['comment_id'],\n",
    "        #     'parent_comment_id': row['parent_comment_id'],\n",
    "        #     'comment_timestamp': row['comment_timestamp'],\n",
    "        #     'level': row['level'],\n",
    "        #     'post_id': row['post_id'],\n",
    "        #     'post_title': row['post_title'],\n",
    "        #     'post_timestamp': row['post_timestamp'],\n",
    "        #     'number_of_comment_votes': row['number_of_comment_votes'],\n",
    "        #     'platform': row['platform'],\n",
    "        #     'sentence': row['sentence'],\n",
    "        #     'previous_sentence': row['previous_sentence'],\n",
    "        #     'sentence_idx': row['sentence_idx'],\n",
    "        #     'contains_trump_mentions': row['contains_trump_mentions'],\n",
    "        #     'contains_kamala_mentions': row['contains_kamala_mentions'],\n",
    "        #     'is_subjective': row['is_subjective'],\n",
    "        #     'entity_ids': row['entity_ids'],\n",
    "        #     'entity_category': 'kamala',\n",
    "        #     'entity_id': entity_idx_map['kamala']\n",
    "        # })\n",
    "    if entity_labels[1] == 1:\n",
    "        dict_row = dict()\n",
    "        for col in df.columns:\n",
    "            dict_row[col] = row[col]\n",
    "            dict_row['entity_category'] = 'trump'\n",
    "            dict_row['entity_id'] = entity_idx_map['trump']\n",
    "        expanded_rows.append(dict_row)\n",
    "\n",
    "        # expanded_rows.append({\n",
    "        #     'comment': row['comment'],\n",
    "        #     'parent_comment': row['parent_comment'],\n",
    "        #     'comment_id': row['comment_id'],\n",
    "        #     'parent_comment_id': row['parent_comment_id'],\n",
    "        #     'comment_timestamp': row['comment_timestamp'],\n",
    "        #     'level': row['level'],\n",
    "        #     'post_id': row['post_id'],\n",
    "        #     'post_title': row['post_title'],\n",
    "        #     'post_timestamp': row['post_timestamp'],\n",
    "        #     'number_of_comment_votes': row['number_of_comment_votes'],\n",
    "        #     'platform': row['platform'],\n",
    "        #     'sentence': row['sentence'],\n",
    "        #     'previous_sentence': row['previous_sentence'],\n",
    "        #     'sentence_idx': row['sentence_idx'],\n",
    "        #     'contains_trump_mentions': row['contains_trump_mentions'],\n",
    "        #     'contains_kamala_mentions': row['contains_kamala_mentions'],\n",
    "        #     'is_subjective': row['is_subjective'],\n",
    "        #     'entity_ids': row['entity_ids'],\n",
    "        #     'entity_category': 'trump',\n",
    "        #     'entity_id': entity_idx_map['trump']\n",
    "        # })\n",
    "    if entity_labels[2] == 1:\n",
    "        dict_row = dict()\n",
    "        for col in df.columns:\n",
    "            dict_row[col] = row[col]\n",
    "            dict_row['entity_category'] = 'others'\n",
    "            dict_row['entity_id'] = entity_idx_map['others']\n",
    "        expanded_rows.append(dict_row)\n",
    "\n",
    "        # expanded_rows.append({\n",
    "        #     'comment': row['comment'],\n",
    "        #     'parent_comment': row['parent_comment'],\n",
    "        #     'comment_id': row['comment_id'],\n",
    "        #     'parent_comment_id': row['parent_comment_id'],\n",
    "        #     'comment_timestamp': row['comment_timestamp'],\n",
    "        #     'level': row['level'],\n",
    "        #     'post_id': row['post_id'],\n",
    "        #     'post_title': row['post_title'],\n",
    "        #     'post_timestamp': row['post_timestamp'],\n",
    "        #     'number_of_comment_votes': row['number_of_comment_votes'],\n",
    "        #     'platform': row['platform'],\n",
    "        #     'sentence': row['sentence'],\n",
    "        #     'previous_sentence': row['previous_sentence'],\n",
    "        #     'sentence_idx': row['sentence_idx'],\n",
    "        #     'contains_trump_mentions': row['contains_trump_mentions'],\n",
    "        #     'contains_kamala_mentions': row['contains_kamala_mentions'],\n",
    "        #     'is_subjective': row['is_subjective'],\n",
    "        #     'entity_ids': row['entity_ids'],\n",
    "        #     'entity_category': 'others',\n",
    "        #     'entity_id': entity_idx_map['others']\n",
    "        # })\n",
    "          \n",
    "\n",
    "# Create a new DataFrame from expanded rows\n",
    "df = pd.DataFrame(expanded_rows)\n",
    "df = df[df_columns + ['entity_category', 'entity_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove sentences without Kamala / Trump\n",
    "df = df[df['entity_category'] != 'others']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aspect Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_aspects = 13\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "        # self.distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "        self.distilbert = AutoModel.from_pretrained(model_name)\n",
    "        self.fc1 = nn.Linear(self.distilbert.config.hidden_size + 1, 256)  # +1 for entity_ids\n",
    "        self.fc2 = nn.Linear(256, num_labels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, entity_ids):\n",
    "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[0][:, 0]  # Use the [CLS] token representation\n",
    "        \n",
    "        entity_ids_expanded = entity_ids.view(entity_ids.size(0), -1)  # Added\n",
    "\n",
    "        # Concatenate pooled_output with entity_ids\n",
    "        # combined_output = torch.cat((pooled_output, entity_ids.unsqueeze(1)), dim=1)\n",
    "        combined_output = torch.cat((pooled_output, entity_ids_expanded), dim=1)\n",
    "        \n",
    "        x = self.fc1(combined_output)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return torch.sigmoid(x)  # Use sigmoid for multi-label classification\n",
    "    \n",
    "def predict_scores(model, tokenizer, dataframe, max_length=512):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    scores = []  # List to hold the scores for each row\n",
    "    \n",
    "    for _, row in dataframe.iterrows():\n",
    "        # Tokenize the text and prepare inputs\n",
    "        tokenized = tokenizer(\n",
    "            row['sentence'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Extract the entity_id and prepare it as a LongTensor\n",
    "        entity_id_tensor = torch.tensor([row['entity_id']], dtype=torch.long).to('cuda')  # Shape: (1, 1)\n",
    "\n",
    "        # Move tokenized inputs to the device\n",
    "        input_ids = tokenized['input_ids'].to('cuda')\n",
    "        attention_mask = tokenized['attention_mask'].to('cuda')\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask, entity_ids=entity_id_tensor)\n",
    "            scores.append(output.cpu().numpy().flatten().tolist())  # Flatten to a list of length 14\n",
    "\n",
    "    # Convert scores to a list of lists and add to DataFrame\n",
    "    dataframe['distil_aspect_scores'] = scores\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def get_aspect_category(x, map):\n",
    "    if sum(x) == 0:\n",
    "        return ['others']\n",
    "    else:\n",
    "        result = []\n",
    "        for idx, value in enumerate(x):\n",
    "            if value == 1:\n",
    "                result.append(map[idx])\n",
    "            \n",
    "        return result\n",
    "\n",
    "model = MultiLabelClassifier(num_labels=num_aspects).to('cuda')\n",
    "model.load_state_dict(torch.load(ASPECT_MODEL_DISTIL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predict_scores(model, tokenizer, df)\n",
    "df['distil_aspect_labels'] = df.distil_aspect_scores.apply(lambda x: np.where(np.array(x) >= 0.35, 1.0, 0.0)) # Returns list of length 13, if all 0 then its others.\n",
    "df['distil_aspect_categories'] = df['distil_aspect_labels'].apply(lambda x: get_aspect_category(x, idx_aspect_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_timestamp</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_timestamp</th>\n",
       "      <th>number_of_comment_votes</th>\n",
       "      <th>sentence</th>\n",
       "      <th>previous_sentence</th>\n",
       "      <th>is_subjective</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>entity_category</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>distil_aspect_scores</th>\n",
       "      <th>distil_aspect_labels</th>\n",
       "      <th>distil_aspect_categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lt2ikja</td>\n",
       "      <td>1g88apd</td>\n",
       "      <td>It was all STAGED!! Trump did not work. McDona...</td>\n",
       "      <td>2024-10-20 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did people honestly think trump actually worke...</td>\n",
       "      <td>2024-10-21 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Did people honestly think trump actually worke...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>trump</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.028811892494559288, 0.021681832149624825, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[others]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lsyikl3</td>\n",
       "      <td>1g88apd</td>\n",
       "      <td>It was all STAGED!! Trump did not work. McDona...</td>\n",
       "      <td>2024-10-20 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better than Kamala any day of the week. Both a...</td>\n",
       "      <td>2024-10-21 00:00:00+00:00</td>\n",
       "      <td>-2</td>\n",
       "      <td>Better than Kamala any day of the week.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>kamala</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0308123379945755, 0.019607435911893845, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[voter sentiment]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id  post_id                                         post_title  \\\n",
       "0    lt2ikja  1g88apd  It was all STAGED!! Trump did not work. McDona...   \n",
       "1    lsyikl3  1g88apd  It was all STAGED!! Trump did not work. McDona...   \n",
       "\n",
       "              post_timestamp  parent_comment_id  parent_comment  \\\n",
       "0  2024-10-20 00:00:00+00:00                NaN             NaN   \n",
       "1  2024-10-20 00:00:00+00:00                NaN             NaN   \n",
       "\n",
       "                                             comment  \\\n",
       "0  Did people honestly think trump actually worke...   \n",
       "1  Better than Kamala any day of the week. Both a...   \n",
       "\n",
       "           comment_timestamp  number_of_comment_votes  \\\n",
       "0  2024-10-21 00:00:00+00:00                        1   \n",
       "1  2024-10-21 00:00:00+00:00                       -2   \n",
       "\n",
       "                                            sentence  previous_sentence  \\\n",
       "0  Did people honestly think trump actually worke...                NaN   \n",
       "1            Better than Kamala any day of the week.                NaN   \n",
       "\n",
       "   is_subjective entity_ids entity_category  entity_id  \\\n",
       "0              1  [0, 1, 0]           trump          1   \n",
       "1              1  [1, 0, 0]          kamala          0   \n",
       "\n",
       "                                distil_aspect_scores  \\\n",
       "0  [0.028811892494559288, 0.021681832149624825, 0...   \n",
       "1  [0.0308123379945755, 0.019607435911893845, 0.0...   \n",
       "\n",
       "                                distil_aspect_labels distil_aspect_categories  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...                 [others]  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        [voter sentiment]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence2'] = df.apply(lambda row: 'entity of interest: ' + row['entity_category'].replace('others', 'neither trump nor kamala') + ' [SEP] ' + row['sentence'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_name = \"facebook/bart-large\" \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained('./model/aspect_model_seq2seq_facebooklargebart_lr5e-5_epochs5_w_additional_val_acc_4683')\n",
    "# model.to('cuda')\n",
    "\n",
    "model_name = 'destonedbob/nusiss-election-project-aspect-seq2seq-model-facebook-bart-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "model.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_aspect_model2b(texts, batch_size=32, return_conf=False, return_aspect_list=False):\n",
    "    predictions = []\n",
    "\n",
    "    # Tokenize texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs.to('cuda')  # Move to GPU\n",
    "\n",
    "        if return_conf:\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                output_ids = model.generate(**inputs, output_scores=True, return_dict_in_generate=True)\n",
    "                predictions.append(output_ids.sequences_scores.cpu().numpy())  # Collect scores\n",
    "        \n",
    "        else:\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                output_ids = model.generate(**inputs)\n",
    "\n",
    "            # Decode the batch of generated sequences\n",
    "            output_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "            if return_aspect_list:\n",
    "                for output_text in output_texts:\n",
    "                    aspect_lst = output_text.split(';')\n",
    "                    aspect_lst_result = [0] * len(aspect_idx_map)\n",
    "                    for aspect in aspect_lst:\n",
    "                        processed_aspect = aspect.lower().strip()\n",
    "                        aspect_id = aspect_idx_map.get(processed_aspect, 9999)\n",
    "                        if aspect_id == 9999:\n",
    "                            print(processed_aspect)\n",
    "                            continue\n",
    "                        aspect_lst_result[aspect_id] = 1\n",
    "\n",
    "                    predictions.append(aspect_lst_result)\n",
    "            else:\n",
    "                predictions.extend(output_texts)  # Append the decoded texts\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "mask = df['distil_aspect_categories'].apply(lambda x: x == ['others'])\n",
    "\n",
    "# Get the sentences where the condition is met\n",
    "texts_to_predict = df.loc[mask, 'sentence2'].tolist()\n",
    "\n",
    "# Call the combined predict function\n",
    "if texts_to_predict:\n",
    "    predicted_aspect_lists = predict_aspect_model2b(texts_to_predict, batch_size=32, return_aspect_list=True)\n",
    "    # Assign the predictions back to the relevant rows using .loc\n",
    "    print(len(texts_to_predict))\n",
    "    print(len(predicted_aspect_lists))\n",
    "    df.loc[mask, 'bart_aspect_labels'] = pd.Series(predicted_aspect_lists, index=df[mask].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aspect_category_model2(x, map):\n",
    "    if type(x) != list:\n",
    "        return np.nan\n",
    "    \n",
    "    if sum(x) == 0:\n",
    "        return ['others']\n",
    "    else:\n",
    "        result = []\n",
    "        for idx, value in enumerate(x):\n",
    "            if value == 1:\n",
    "                result.append(map[idx])\n",
    "            \n",
    "        return result\n",
    "    \n",
    "df['bart_aspect_categories'] = df['bart_aspect_labels'].apply(lambda x: get_aspect_category_model2(x, idx_aspect_map))\n",
    "df['final_aspect_categories'] = df['bart_aspect_categories'].fillna(df['distil_aspect_categories'])\n",
    "df['final_aspect_labels'] = df['bart_aspect_labels'].fillna(df['distil_aspect_labels'].apply(lambda x: [int(each) for each in x] + [1] if sum(x) == 0 else [int(each) for each in x] + [0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.explode('final_aspect_categories').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>post_title</th>\n",
       "      <th>post_timestamp</th>\n",
       "      <th>parent_comment_id</th>\n",
       "      <th>parent_comment</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_timestamp</th>\n",
       "      <th>number_of_comment_votes</th>\n",
       "      <th>sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>entity_category</th>\n",
       "      <th>entity_id</th>\n",
       "      <th>distil_aspect_scores</th>\n",
       "      <th>distil_aspect_labels</th>\n",
       "      <th>distil_aspect_categories</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>bart_aspect_labels</th>\n",
       "      <th>bart_aspect_categories</th>\n",
       "      <th>final_aspect_categories</th>\n",
       "      <th>final_aspect_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lt2ikja</td>\n",
       "      <td>1g88apd</td>\n",
       "      <td>It was all STAGED!! Trump did not work. McDona...</td>\n",
       "      <td>2024-10-20 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did people honestly think trump actually worke...</td>\n",
       "      <td>2024-10-21 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Did people honestly think trump actually worke...</td>\n",
       "      <td>...</td>\n",
       "      <td>trump</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.028811892494559288, 0.021681832149624825, 0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[others]</td>\n",
       "      <td>entity of interest: trump [SEP] Did people hon...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[controversies]</td>\n",
       "      <td>controversies</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lsyikl3</td>\n",
       "      <td>1g88apd</td>\n",
       "      <td>It was all STAGED!! Trump did not work. McDona...</td>\n",
       "      <td>2024-10-20 00:00:00+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Better than Kamala any day of the week. Both a...</td>\n",
       "      <td>2024-10-21 00:00:00+00:00</td>\n",
       "      <td>-2</td>\n",
       "      <td>Better than Kamala any day of the week.</td>\n",
       "      <td>...</td>\n",
       "      <td>kamala</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0308123379945755, 0.019607435911893845, 0.0...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[voter sentiment]</td>\n",
       "      <td>entity of interest: kamala [SEP] Better than K...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>voter sentiment</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  comment_id  post_id                                         post_title  \\\n",
       "0    lt2ikja  1g88apd  It was all STAGED!! Trump did not work. McDona...   \n",
       "1    lsyikl3  1g88apd  It was all STAGED!! Trump did not work. McDona...   \n",
       "\n",
       "              post_timestamp  parent_comment_id  parent_comment  \\\n",
       "0  2024-10-20 00:00:00+00:00                NaN             NaN   \n",
       "1  2024-10-20 00:00:00+00:00                NaN             NaN   \n",
       "\n",
       "                                             comment  \\\n",
       "0  Did people honestly think trump actually worke...   \n",
       "1  Better than Kamala any day of the week. Both a...   \n",
       "\n",
       "           comment_timestamp  number_of_comment_votes  \\\n",
       "0  2024-10-21 00:00:00+00:00                        1   \n",
       "1  2024-10-21 00:00:00+00:00                       -2   \n",
       "\n",
       "                                            sentence  ...  entity_category  \\\n",
       "0  Did people honestly think trump actually worke...  ...            trump   \n",
       "1            Better than Kamala any day of the week.  ...           kamala   \n",
       "\n",
       "   entity_id                               distil_aspect_scores  \\\n",
       "0          1  [0.028811892494559288, 0.021681832149624825, 0...   \n",
       "1          0  [0.0308123379945755, 0.019607435911893845, 0.0...   \n",
       "\n",
       "                                distil_aspect_labels  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "   distil_aspect_categories  \\\n",
       "0                  [others]   \n",
       "1         [voter sentiment]   \n",
       "\n",
       "                                           sentence2  \\\n",
       "0  entity of interest: trump [SEP] Did people hon...   \n",
       "1  entity of interest: kamala [SEP] Better than K...   \n",
       "\n",
       "                           bart_aspect_labels bart_aspect_categories  \\\n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]        [controversies]   \n",
       "1                                         NaN                    NaN   \n",
       "\n",
       "  final_aspect_categories                         final_aspect_labels  \n",
       "0           controversies  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "1         voter sentiment  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['final_aspect_ids'] = df['final_aspect_categories'].apply(lambda x: aspect_idx_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AspectBasedSentimentModel(\n",
       "  (bert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (entity_embedding): Embedding(3, 256)\n",
       "  (aspect_embedding): Embedding(14, 256)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (entity_labels_embedding): Embedding(2, 256)\n",
       "  (aspect_labels_embedding): Embedding(2, 256)\n",
       "  (classifier): Linear(in_features=5632, out_features=514, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (output_layer): Linear(in_features=514, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class AspectBasedSentimentModel(nn.Module):\n",
    "    def __init__(self, pretrained_model_name):\n",
    "        super(AspectBasedSentimentModel, self).__init__()\n",
    "\n",
    "        num_extra_dims = 2\n",
    "        self.config = AutoConfig.from_pretrained(pretrained_model_name)\n",
    "        self.bert = AutoModel.from_pretrained(pretrained_model_name, config=self.config)\n",
    "\n",
    "        for param in self.bert.transformer.layer[:4].parameters():  # Freeze first 4 layers\n",
    "            param.requires_grad = False\n",
    "\n",
    "        num_hidden_size = self.bert.config.hidden_size \n",
    "        self.entity_embedding = nn.Embedding(num_embeddings=3, embedding_dim=256)\n",
    "        self.aspect_embedding = nn.Embedding(num_embeddings=14, embedding_dim=256)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        # self.classifier = torch.nn.Linear(num_hidden_size+num_extra_dims, 3)\n",
    "        # self.classifier = torch.nn.Linear(num_hidden_size + 1028 + 1028, 3)\n",
    "        self.entity_labels_embedding = nn.Embedding(num_embeddings=2, embedding_dim=256)  # Embedding for binary 0/1 values\n",
    "        self.aspect_labels_embedding = nn.Embedding(num_embeddings=2, embedding_dim=256)  # Embedding for binary 0/1 values\n",
    "\n",
    "        self.classifier = nn.Linear(num_hidden_size + 256 + 256 + (256 * 3) + (256 * 14), 514)  # First hidden layer\n",
    "        self.relu = nn.ReLU()  # Activation function\n",
    "        # Second linear layer (output layer)\n",
    "        self.output_layer = nn.Linear(514, 3)  # Final layer to output class probabilities\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, entity_cat, aspect_cat, entity_labels, aspect_labels):\n",
    "        \n",
    "        hidden_states = self.bert(input_ids=input_ids, attention_mask=attention_mask)  # [batch size, sequence length, hidden size]\n",
    "        cls_embeddings = hidden_states.last_hidden_state[:, 0, :] # [batch size, hidden size]\n",
    "        # concat = torch.cat((cls_embeddings, entity_cat.unsqueeze(1), aspect_cat.unsqueeze(1)), axis=1) # [batch size, hidden size+num extra dims]\n",
    "        \n",
    "        entity_embed = self.entity_embedding(entity_cat.type(torch.IntTensor).to('cuda'))\n",
    "        aspect_embed = self.aspect_embedding(aspect_cat.type(torch.IntTensor).to('cuda'))\n",
    "        # print((cls_embeddings.shape, entity_embed.shape, aspect_embed.shape))\n",
    "        entity_labels_embed = self.entity_labels_embedding(entity_labels.type(torch.LongTensor).to('cuda')).view(entity_labels.shape[0], -1)  # Flatten [batch_size, 3, 50] to [batch_size, 150]\n",
    "        aspect_labels_embed = self.aspect_labels_embedding(aspect_labels.type(torch.LongTensor).to('cuda')).view(aspect_labels.shape[0], -1)  # Flatten [batch_size, 14, 50] to [batch_size, 700]\n",
    "        \n",
    "        # Concatenate embeddings with CLS token output\n",
    "        concat = torch.cat((cls_embeddings, entity_embed, aspect_embed, entity_labels_embed, aspect_labels_embed), axis=1)\n",
    "        hidden_output = self.relu(self.classifier(self.dropout(concat)))  # [batch size, 128]\n",
    "\n",
    "        # logits = self.output_layer(self.dropout(hidden_output))  # [batch size, num labels]\n",
    "        logits = self.output_layer(hidden_output)  # [batch size, num labels]\n",
    "\n",
    "\n",
    "        # logits = self.classifier(self.dropout(concat)) # [batch size, num labels]\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "def predict_sentiment(row, tokenizer, model, return_conf=False, return_both=False):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(row['sentence'], return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to('cuda')\n",
    "    attention_mask = inputs['attention_mask'].to('cuda')\n",
    "    entity_cat = torch.tensor([row['entity_id']]).to('cuda')\n",
    "    aspect_cat = torch.tensor([row['final_aspect_ids']]).to('cuda')\n",
    "    entity_labels = torch.tensor([row['entity_ids']]).to('cuda')\n",
    "    aspect_labels = torch.tensor([row['final_aspect_labels']]).to('cuda')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask, entity_cat, aspect_cat, entity_labels, aspect_labels)\n",
    "    \n",
    "    if return_both:\n",
    "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "        return (predicted_label - 1, torch.max(logits).item())\n",
    "    \n",
    "    if return_conf:\n",
    "        return torch.max(logits).item()\n",
    "    else:\n",
    "        predicted_label = torch.argmax(logits, dim=-1).item()\n",
    "        return predicted_label - 1\n",
    "    \n",
    "    \n",
    "model = torch.load(SENTIMENT_MODEL_DISTIL)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distil_sentiment_prediction_and_confidence_score'] = df.apply(lambda x: predict_sentiment(x, tokenizer, model, return_both=True), axis=1)\n",
    "df['distil_sentiment_prediction'] = df['distil_sentiment_prediction_and_confidence_score'].apply(lambda x: x[0])\n",
    "df['distil_sentiment_confidence_score'] = df['distil_sentiment_prediction_and_confidence_score'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_for_sentiment_seq2seq(row):\n",
    "    return f\"entity of interest: {row['entity_category'].replace('others', 'neither trump nor kamala')} [SEP] aspect of interest: {row['final_aspect_categories']} [SEP] {row['sentence']}\"\n",
    "\n",
    "\n",
    "def predict_sentiment_model2_batch(texts, batch_size=32, return_conf=False, return_both=False):\n",
    "    predictions = []\n",
    "    confidences = []\n",
    "\n",
    "    # Tokenize texts in batches\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        # Tokenize the batch\n",
    "        inputs = tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = inputs.to('cuda')  # Move to GPU\n",
    "\n",
    "        if return_both:\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                output_ids = model.generate(**inputs, output_scores=True, return_dict_in_generate=True)\n",
    "                confidences.extend(output_ids.sequences_scores.cpu().tolist())\n",
    "\n",
    "            output_texts = tokenizer.batch_decode(output_ids.sequences, skip_special_tokens=True)\n",
    "            for output_text in output_texts:\n",
    "                try:\n",
    "                    # Extract label and map it to sentiment index\n",
    "                    final_label = sentiment_idx_map[output_text.split(': ')[1]] - 1\n",
    "                except:\n",
    "                    print(output_text)\n",
    "                    final_label = 0\n",
    "                predictions.append(final_label)\n",
    "            \n",
    "        elif return_conf:\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                output_ids = model.generate(**inputs, output_scores=True, return_dict_in_generate=True)\n",
    "                # Collect sequence scores for confidence\n",
    "                predictions.extend(output_ids.sequences_scores.cpu().tolist())\n",
    "        else:\n",
    "            with torch.no_grad():  # Disable gradient calculation\n",
    "                output_ids = model.generate(**inputs)\n",
    "\n",
    "            # Decode the batch of generated sequences\n",
    "            output_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "\n",
    "            for output_text in output_texts:\n",
    "                try:\n",
    "                    # Extract label and map it to sentiment index\n",
    "                    final_label = sentiment_idx_map[output_text.split(': ')[1]] - 1\n",
    "                except:\n",
    "                    print(output_text)\n",
    "                    final_label = 0\n",
    "                predictions.append(final_label)\n",
    "\n",
    "    if return_both:\n",
    "        return predictions, confidences\n",
    "    else:\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "df['sentence3'] = df.apply(create_sentence_for_sentiment_seq2seq, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('destonedbob/nusiss-election-project-sentiment-seq2seq-model-facebook-bart-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('destonedbob/nusiss-election-project-sentiment-seq2seq-model-facebook-bart-large')\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_predict = df.sentence3.tolist()\n",
    "if texts_to_predict:\n",
    "    predicted_data = predict_sentiment_model2_batch(texts_to_predict, batch_size=32, return_both=True)\n",
    "df['seq2seq_sentiment_prediction'] = pd.Series(predicted_data[0])\n",
    "df['seq2seq_sentiment_confidence_score'] = pd.Series(predicted_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_sentiment_pred(row):\n",
    "    seq2seq_conf = row['seq2seq_sentiment_confidence_score']\n",
    "    distil_conf = row['distil_sentiment_confidence_score']\n",
    "\n",
    "    if distil_conf >= 0.85: # 0.6\n",
    "        return row['distil_sentiment_prediction']\n",
    "    elif seq2seq_conf >= -0.36: # -0.32499999999999996\n",
    "        return row['seq2seq_sentiment_prediction']\n",
    "    else:\n",
    "        return row['distil_sentiment_prediction']\n",
    "\n",
    "df['final_sentiment_prediction'] = df.apply(get_final_sentiment_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['comment_id', 'post_id', 'post_title', 'post_timestamp',\n",
       "       'parent_comment_id', 'parent_comment', 'comment', 'comment_timestamp',\n",
       "       'number_of_comment_votes', 'sentence', 'previous_sentence',\n",
       "       'is_subjective', 'entity_ids', 'entity_category', 'entity_id',\n",
       "       'distil_aspect_scores', 'distil_aspect_labels',\n",
       "       'distil_aspect_categories', 'sentence2', 'bart_aspect_labels',\n",
       "       'bart_aspect_categories', 'final_aspect_categories',\n",
       "       'final_aspect_labels', 'final_aspect_ids',\n",
       "       'distil_sentiment_prediction_and_confidence_score',\n",
       "       'distil_sentiment_prediction', 'distil_sentiment_confidence_score',\n",
       "       'sentence3', 'seq2seq_sentiment_prediction',\n",
       "       'seq2seq_sentiment_confidence_score', 'final_sentiment_prediction'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>entity_ids</th>\n",
       "      <th>final_aspect_labels</th>\n",
       "      <th>entity_category</th>\n",
       "      <th>final_aspect_categories</th>\n",
       "      <th>final_sentiment_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Did people honestly think trump actually worke...</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>trump</td>\n",
       "      <td>controversies</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Better than Kamala any day of the week.</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "      <td>kamala</td>\n",
       "      <td>voter sentiment</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence entity_ids  \\\n",
       "0  Did people honestly think trump actually worke...  [0, 1, 0]   \n",
       "1            Better than Kamala any day of the week.  [1, 0, 0]   \n",
       "\n",
       "                          final_aspect_labels entity_category  \\\n",
       "0  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]           trump   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]          kamala   \n",
       "\n",
       "  final_aspect_categories  final_sentiment_prediction  \n",
       "0           controversies                          -1  \n",
       "1         voter sentiment                           1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['sentence', 'entity_ids', 'final_aspect_labels', 'entity_category', 'final_aspect_categories',  'final_sentiment_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
